{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c77d3b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36e8f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data = pd.read_json('Appliances_5.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b338a003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>style</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>08 22, 2013</td>\n",
       "      <td>A34A1UP40713F8</td>\n",
       "      <td>B00009W3I4</td>\n",
       "      <td>{'Style:': ' Dryer Vent'}</td>\n",
       "      <td>James. Backus</td>\n",
       "      <td>I like this as a vent as well as something tha...</td>\n",
       "      <td>Great product</td>\n",
       "      <td>1377129600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>02 8, 2016</td>\n",
       "      <td>A1AHW6I678O6F2</td>\n",
       "      <td>B00009W3PA</td>\n",
       "      <td>{'Size:': ' 6-Foot'}</td>\n",
       "      <td>kevin.</td>\n",
       "      <td>good item</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1454889600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>08 5, 2015</td>\n",
       "      <td>A8R48NKTGCJDQ</td>\n",
       "      <td>B00009W3PA</td>\n",
       "      <td>{'Size:': ' 6-Foot'}</td>\n",
       "      <td>CDBrannom</td>\n",
       "      <td>Fit my new LG dryer perfectly.</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1438732800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>04 24, 2015</td>\n",
       "      <td>AR3OHHHW01A8E</td>\n",
       "      <td>B00009W3PA</td>\n",
       "      <td>{'Size:': ' 6-Foot'}</td>\n",
       "      <td>Calvin E Reames</td>\n",
       "      <td>Good value for electric dryers</td>\n",
       "      <td>Perfect size</td>\n",
       "      <td>1429833600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>03 21, 2015</td>\n",
       "      <td>A2CIEGHZ7L1WWR</td>\n",
       "      <td>B00009W3PA</td>\n",
       "      <td>{'Size:': ' 6-Foot'}</td>\n",
       "      <td>albert j. kong</td>\n",
       "      <td>Price and delivery was excellent.</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1426896000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified   reviewTime      reviewerID        asin  \\\n",
       "0        5      True  08 22, 2013  A34A1UP40713F8  B00009W3I4   \n",
       "1        5      True   02 8, 2016  A1AHW6I678O6F2  B00009W3PA   \n",
       "2        5      True   08 5, 2015   A8R48NKTGCJDQ  B00009W3PA   \n",
       "3        5      True  04 24, 2015   AR3OHHHW01A8E  B00009W3PA   \n",
       "4        5      True  03 21, 2015  A2CIEGHZ7L1WWR  B00009W3PA   \n",
       "\n",
       "                       style     reviewerName  \\\n",
       "0  {'Style:': ' Dryer Vent'}    James. Backus   \n",
       "1       {'Size:': ' 6-Foot'}           kevin.   \n",
       "2       {'Size:': ' 6-Foot'}        CDBrannom   \n",
       "3       {'Size:': ' 6-Foot'}  Calvin E Reames   \n",
       "4       {'Size:': ' 6-Foot'}   albert j. kong   \n",
       "\n",
       "                                          reviewText        summary  \\\n",
       "0  I like this as a vent as well as something tha...  Great product   \n",
       "1                                          good item     Five Stars   \n",
       "2                     Fit my new LG dryer perfectly.     Five Stars   \n",
       "3                     Good value for electric dryers   Perfect size   \n",
       "4                  Price and delivery was excellent.     Five Stars   \n",
       "\n",
       "   unixReviewTime vote image  \n",
       "0      1377129600  NaN   NaN  \n",
       "1      1454889600  NaN   NaN  \n",
       "2      1438732800  NaN   NaN  \n",
       "3      1429833600  NaN   NaN  \n",
       "4      1426896000  NaN   NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b52fed1",
   "metadata": {},
   "source": [
    "### Filter our Column we want to analyse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e6a554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Filter_review = review_data[['overall','reviewText','summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51ed59cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>I like this as a vent as well as something tha...</td>\n",
       "      <td>Great product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>good item</td>\n",
       "      <td>Five Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Fit my new LG dryer perfectly.</td>\n",
       "      <td>Five Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Good value for electric dryers</td>\n",
       "      <td>Perfect size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Price and delivery was excellent.</td>\n",
       "      <td>Five Stars</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall                                         reviewText        summary\n",
       "0        5  I like this as a vent as well as something tha...  Great product\n",
       "1        5                                          good item     Five Stars\n",
       "2        5                     Fit my new LG dryer perfectly.     Five Stars\n",
       "3        5                     Good value for electric dryers   Perfect size\n",
       "4        5                  Price and delivery was excellent.     Five Stars"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Filter_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff9dffa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall\n",
      "5    0.707949\n",
      "3    0.184892\n",
      "4    0.097497\n",
      "2    0.005709\n",
      "1    0.003953\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(Filter_review.overall.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae86b659",
   "metadata": {},
   "source": [
    "overall we have a high rating of 5 & 4 which shows how our customer like the product but we have also bad reviews which can help us to improve the product for everyone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9632b6",
   "metadata": {},
   "source": [
    "## Corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b7ed6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of our corpus is: 2277 \n",
      "\n",
      "The first Review in our corpus is:\n",
      " I like this as a vent as well as something that will keep house warmer in winter.  I sanded it and then painted it the same color as the house.  Looks great.\n"
     ]
    }
   ],
   "source": [
    "corpus =  Filter_review.reviewText.tolist()\n",
    "\n",
    "print(\"The size of our corpus is:\", len(corpus), \"\\n\")\n",
    "print(\"The first Review in our corpus is:\\n\", corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbb3a62",
   "metadata": {},
   "source": [
    "##  Tokenization\n",
    "\n",
    "tokenization is a process of dividing documents(corpus) into individual meaningful tokens that make a document (corpus).\n",
    "nltk is a wonderful package that provides tokenization tools that perform very well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4777ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/marshal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f67b9956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Review:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'We would give less than 1 star if possible DONT BUY THIS PRODUCT.  The ice machine stopped working four hours after we used it the first time.  We notified New Air and they stated they would not honor their one year warranty because \"an authorized dealer didn\\'t sell\".  We bought this product on Amazon and never even thought we would have to cross check our purchase with the manufacturer.  NewAir does not stand by their products and they will use any method to get out of honoring their warranty.  I have a $200 piece of junk now, I will NEVER buy another NewAir product again.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "print ('Original Review:')\n",
    "Filter_review.reviewText[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57726767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence Tokenized:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['We would give less than 1 star if possible DONT BUY THIS PRODUCT.',\n",
       " 'The ice machine stopped working four hours after we used it the first time.',\n",
       " 'We notified New Air and they stated they would not honor their one year warranty because \"an authorized dealer didn\\'t sell\".',\n",
       " 'We bought this product on Amazon and never even thought we would have to cross check our purchase with the manufacturer.',\n",
       " 'NewAir does not stand by their products and they will use any method to get out of honoring their warranty.',\n",
       " 'I have a $200 piece of junk now, I will NEVER buy another NewAir product again.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('sentence Tokenized:')\n",
    "sent_tokenize(Filter_review.reviewText[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ac3071",
   "metadata": {},
   "source": [
    "Notice that the resulting output is a list of sentences. The sent_tokenize uses heuristic rules about the English language to separate sentences. In this case, look for periods and punctuation that separate complete sentences i.e. !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbc413e",
   "metadata": {},
   "source": [
    "### Punckt Sentence Tokenizer\n",
    "\n",
    "Another sentence tokenizer is the PunktSentenceTokenizer which performs similarly and may be more effective in some instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96921020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We would give less than 1 star if possible DONT BUY THIS PRODUCT.',\n",
       " 'The ice machine stopped working four hours after we used it the first time.',\n",
       " 'We notified New Air and they stated they would not honor their one year warranty because \"an authorized dealer didn\\'t sell\".',\n",
       " 'We bought this product on Amazon and never even thought we would have to cross check our purchase with the manufacturer.',\n",
       " 'NewAir does not stand by their products and they will use any method to get out of honoring their warranty.',\n",
       " 'I have a $200 piece of junk now, I will NEVER buy another NewAir product again.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "punkt_sent_tokenizer = PunktSentenceTokenizer()\n",
    "punkt_sent_tokenizer.tokenize( Filter_review.reviewText[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53694571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word Tokenizer: ['I', 'like', 'this', 'as', 'a', 'vent', 'as', 'well', 'as', 'something', 'that', 'will', 'keep', 'house', 'warmer', 'in', 'winter', '.', 'I', 'sanded', 'it', 'and', 'then', 'painted', 'it', 'the', 'same', 'color', 'as', 'the', 'house', '.', 'Looks', 'great', '.']\n"
     ]
    }
   ],
   "source": [
    " from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "    \n",
    "print(\"word Tokenizer:\", word_tokenize(corpus[0]))\n",
    "#print(\"word punct Tokenizer:\", wordpunct_tokenize(corpus[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d03fd9",
   "metadata": {},
   "source": [
    "The output above displays the words generated by both word tokenizers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689810c4",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming is the process of reducing texts to their root words. This feature is powerful as it allows us to reduce the number of tokens in a document or corpus significantly while retaining the context as much as possible. In the example below, we implement two stemmers on the set of variants with the same root.\n",
    "\n",
    "Let's see the examples of stemmers in nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fb3d71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "\n",
    "porter_stem = PorterStemmer()\n",
    "snowb_stem = SnowballStemmer('english') # Need to initialize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9738a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('love', 'love', 'love')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stem.stem(\"loved\"), porter_stem.stem(\"lovely\"), porter_stem.stem(\"loveness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a498384b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('amaz', 'amaz', 'amaz', 'amaz')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowb_stem.stem('amazing'), snowb_stem.stem('amazed'), snowb_stem.stem('amazes'), snowb_stem.stem('amazingly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d989a5",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "Lemmatization also performs stemming however, it uses vocabulary and morphology to return the base of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aff7e16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/marshal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "faab4326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('wolf', 'say', 'be')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatizer.lemmatize(\"wolves\"), lemmatizer.lemmatize(\"saying\", pos='v'), lemmatizer.lemmatize(\"is\", pos='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaffb0f",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "Stopwords are words that are part of the grammatical structure of the language but do not carry much semantic meaning to the text. These are often frequent words like 'is' and 'the' that whether they are in or out of the text, the meaning does not change. In NLP, we often deal with stopwords by removing them or selecting what to retain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "605d5aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/marshal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f60088b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Stop words: the restaurant at the city served amazing food \n",
      "\n",
      "Without Stop words: restaurant city served amazing food\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"the restaurant at the city served amazing food\"\n",
    "print(\"With Stop words:\", sentence, \"\\n\" )\n",
    "\n",
    "without_stopwords = [word for word in word_tokenize(sentence) if word not in stopwords.words('english')]\n",
    "print('Without Stop words:', ' '.join(without_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e32d0f",
   "metadata": {},
   "source": [
    "## Combining All Preprocessing Steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ac0ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c0b19d",
   "metadata": {},
   "source": [
    "### Text Cleaning:\n",
    "\n",
    "        - Remove Punctuation\n",
    "        - Remove Numbers\n",
    "        - Tokenize Text\n",
    "        - Stem Text\n",
    "        - Remove Stopwords\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5cf0fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "def cleaningText(text):\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text) # Remove non-alphabetic character\n",
    "    text = re.sub(\"[0-9]+\", \"\", text) # Remove Numbers\n",
    "    text = [ porter_stemmer.stem(word.lower()) for word in word_tokenize(text) if word not in stopwords.words('english') ]\n",
    "    return \" \".join(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "987c0e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the restaurant realli amaz servic great food'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaningText(\"The restaurantant has really amazing service and great food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a84a5adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we would give less star possibl dont buy thi product the ice machin stop work four hour use first time we notifi new air state would honor one year warranti author dealer sell we bought product amazon never even thought would cross check purchas manufactur newair stand product use method get honor warranti i piec junk i never buy anoth newair product'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaningText(\"We would give less than 1 star if possible DONT BUY THIS PRODUCT.  The ice machine stopped working four hours after we used it the first time.  We notified New Air and they stated they would not honor their one year warranty because an authorized dealer didn't sell.  We bought this product on Amazon and never even thought we would have to cross check our purchase with the manufacturer.  NewAir does not stand by their products and they will use any method to get out of honoring their warranty.  I have a $200 piece of junk now, I will NEVER buy another NewAir product again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063c9abf",
   "metadata": {},
   "source": [
    "## running the Preprocesing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdd41c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40719/2080681682.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Filter_review['clean_review'] = Filter_review.reviewText.apply( lambda x: cleaningText( str(x)))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I like this as a vent as well as something tha...</td>\n",
       "      <td>i like vent well someth keep hous warmer winte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>good item</td>\n",
       "      <td>good item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fit my new LG dryer perfectly.</td>\n",
       "      <td>fit new lg dryer perfectli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good value for electric dryers</td>\n",
       "      <td>good valu electr dryer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Price and delivery was excellent.</td>\n",
       "      <td>price deliveri excel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  \\\n",
       "0  I like this as a vent as well as something tha...   \n",
       "1                                          good item   \n",
       "2                     Fit my new LG dryer perfectly.   \n",
       "3                     Good value for electric dryers   \n",
       "4                  Price and delivery was excellent.   \n",
       "\n",
       "                                        clean_review  \n",
       "0  i like vent well someth keep hous warmer winte...  \n",
       "1                                          good item  \n",
       "2                         fit new lg dryer perfectli  \n",
       "3                             good valu electr dryer  \n",
       "4                               price deliveri excel  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Filter_review['clean_review'] = Filter_review.reviewText.apply( lambda x: cleaningText( str(x)))\n",
    "Filter_review[['reviewText','clean_review']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5313db",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "This section deals with moving from text to Numerical representation for which analysis and models can be developed \n",
    "\n",
    "Techniques to Develop Features\n",
    "\n",
    "    1.N-grams\n",
    "    2.Bag of Words\n",
    "    3.TF-IDF: Term Frequency - Inverse Document Frequency\n",
    "\n",
    "Feature to Matrix Transformation\n",
    "\n",
    "    1.CountVectorizer\n",
    "    2.TfidfVectorizer\n",
    "    3.Features to Matrix\n",
    "    4.Pickling Vectorizer and Features Dataframe\n",
    "\n",
    "Some datasets to explore: https://nijianmo.github.io/amazon/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc908f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "def cleaningText(text):\n",
    "    \"\"\"\n",
    "    Text Cleaning:\n",
    "        - Remove Punctuation\n",
    "        - Remove Numbers\n",
    "        - Tokenize Text\n",
    "        - Stem Text\n",
    "        - Remove Stopwords\n",
    "    \"\"\" \n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text) # Remove Punctuation\n",
    "    text = re.sub(\"[0-9]+\", \"\", text) # Remove Numbers\n",
    "    text = [ porter_stemmer.stem(word.lower()) for word in word_tokenize(text) if word not in stopwords.words('english') ]\n",
    "    return \" \".join(text)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebc9d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_data = pd.read_json('AMAZON_FASHION_5.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "462faffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3176 entries, 0 to 3175\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   overall         3176 non-null   int64  \n",
      " 1   verified        3176 non-null   bool   \n",
      " 2   reviewTime      3176 non-null   object \n",
      " 3   reviewerID      3176 non-null   object \n",
      " 4   asin            3176 non-null   object \n",
      " 5   style           3107 non-null   object \n",
      " 6   reviewerName    3176 non-null   object \n",
      " 7   reviewText      3160 non-null   object \n",
      " 8   summary         3176 non-null   object \n",
      " 9   unixReviewTime  3176 non-null   int64  \n",
      " 10  vote            297 non-null    float64\n",
      " 11  image           106 non-null    object \n",
      "dtypes: bool(1), float64(1), int64(2), object(8)\n",
      "memory usage: 276.2+ KB\n"
     ]
    }
   ],
   "source": [
    "fashion_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dccebe0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>style</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>09 4, 2015</td>\n",
       "      <td>ALJ66O1Y6SLHA</td>\n",
       "      <td>B000K2PJ4K</td>\n",
       "      <td>{'Size:': ' Big Boys', 'Color:': ' Blue/Orange'}</td>\n",
       "      <td>Tonya B.</td>\n",
       "      <td>Great product and price!</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1441324800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>09 4, 2015</td>\n",
       "      <td>ALJ66O1Y6SLHA</td>\n",
       "      <td>B000K2PJ4K</td>\n",
       "      <td>{'Size:': ' Big Boys', 'Color:': ' Black (3746...</td>\n",
       "      <td>Tonya B.</td>\n",
       "      <td>Great product and price!</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1441324800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>09 4, 2015</td>\n",
       "      <td>ALJ66O1Y6SLHA</td>\n",
       "      <td>B000K2PJ4K</td>\n",
       "      <td>{'Size:': ' Big Boys', 'Color:': ' Blue/Gray L...</td>\n",
       "      <td>Tonya B.</td>\n",
       "      <td>Great product and price!</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1441324800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>09 4, 2015</td>\n",
       "      <td>ALJ66O1Y6SLHA</td>\n",
       "      <td>B000K2PJ4K</td>\n",
       "      <td>{'Size:': ' Big Boys', 'Color:': ' Blue (37867...</td>\n",
       "      <td>Tonya B.</td>\n",
       "      <td>Great product and price!</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1441324800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>09 4, 2015</td>\n",
       "      <td>ALJ66O1Y6SLHA</td>\n",
       "      <td>B000K2PJ4K</td>\n",
       "      <td>{'Size:': ' Big Boys', 'Color:': ' Blue/Pink'}</td>\n",
       "      <td>Tonya B.</td>\n",
       "      <td>Great product and price!</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1441324800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified  reviewTime     reviewerID        asin  \\\n",
       "0        5      True  09 4, 2015  ALJ66O1Y6SLHA  B000K2PJ4K   \n",
       "1        5      True  09 4, 2015  ALJ66O1Y6SLHA  B000K2PJ4K   \n",
       "2        5      True  09 4, 2015  ALJ66O1Y6SLHA  B000K2PJ4K   \n",
       "3        5      True  09 4, 2015  ALJ66O1Y6SLHA  B000K2PJ4K   \n",
       "4        5      True  09 4, 2015  ALJ66O1Y6SLHA  B000K2PJ4K   \n",
       "\n",
       "                                               style reviewerName  \\\n",
       "0   {'Size:': ' Big Boys', 'Color:': ' Blue/Orange'}     Tonya B.   \n",
       "1  {'Size:': ' Big Boys', 'Color:': ' Black (3746...     Tonya B.   \n",
       "2  {'Size:': ' Big Boys', 'Color:': ' Blue/Gray L...     Tonya B.   \n",
       "3  {'Size:': ' Big Boys', 'Color:': ' Blue (37867...     Tonya B.   \n",
       "4     {'Size:': ' Big Boys', 'Color:': ' Blue/Pink'}     Tonya B.   \n",
       "\n",
       "                 reviewText     summary  unixReviewTime  vote image  \n",
       "0  Great product and price!  Five Stars      1441324800   NaN   NaN  \n",
       "1  Great product and price!  Five Stars      1441324800   NaN   NaN  \n",
       "2  Great product and price!  Five Stars      1441324800   NaN   NaN  \n",
       "3  Great product and price!  Five Stars      1441324800   NaN   NaN  \n",
       "4  Great product and price!  Five Stars      1441324800   NaN   NaN  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fashion_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915b549a",
   "metadata": {},
   "source": [
    "## Applying cleaningText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "289f696f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great product and price!</td>\n",
       "      <td>great product price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Great product and price!</td>\n",
       "      <td>great product price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great product and price!</td>\n",
       "      <td>great product price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Great product and price!</td>\n",
       "      <td>great product price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great product and price!</td>\n",
       "      <td>great product price</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 reviewText         clean_review\n",
       "0  Great product and price!  great product price\n",
       "1  Great product and price!  great product price\n",
       "2  Great product and price!  great product price\n",
       "3  Great product and price!  great product price\n",
       "4  Great product and price!  great product price"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# running the preprocesing_step\n",
    "fashion_data['clean_review'] = fashion_data.reviewText.apply( lambda x: cleaningText( str(x)) )\n",
    "fashion_data[['reviewText', 'clean_review']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7c060e",
   "metadata": {},
   "source": [
    "## 1. N-Grams\n",
    "\n",
    "N-grams is a process of tokenizing a body of text sequentially to the nth value. When we used word_tokenize, we were effectively performing 1-gram tokenization. Alternatively, we can choose an n-gram that may combine tokens that are more meaningful together i.e. \"New York\" or \"Thank You\". Let's see this as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37e9ec17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bremen is',\n",
       " 'is really',\n",
       " 'really a',\n",
       " 'a cold',\n",
       " 'cold city',\n",
       " 'city to',\n",
       " 'to live',\n",
       " 'live without',\n",
       " 'without a',\n",
       " 'a big',\n",
       " 'big jacket']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "document = \"Bremen is really a cold city to live without a big jacket\"\n",
    "\n",
    "[' '.join(gram) for gram in ngrams(word_tokenize(document), 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5323086d",
   "metadata": {},
   "source": [
    "N-grams have the advantage of capturing sentiments like \"not bad\" or \"very good\" into a singular token which can be an effective feature for analysis and modeling as compared to individual tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e343399d",
   "metadata": {},
   "source": [
    "## 2. Bag of Words\n",
    "\n",
    "The bag of words is a process that generates features by collecting all the tokens in the corpus and placing them in a bag, thereby creating the vocabulary for the corpus. From this vocabulary, one-hot encoding can be applied to determine the presence or absence of each vocabulary in a document, thus creating features.\n",
    "\n",
    "Now, let's illustrate this process with a simple corpus consisting of 5 short documents below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4fd6db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [ \"the restaurant had great food\",\n",
    "           \"i love python programming\",\n",
    "           \"i prefer R to python\",\n",
    "           \"computers are fun to use\",\n",
    "           \"i did not like the movie\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1eadc71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marshal/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bows_counter = CountVectorizer( analyzer='word',            # Word level vectorizer\n",
    "                                lowercase=True,             # Lower case the text\n",
    "                                ngram_range=(1, 1),         # Create 1 n-grams\n",
    "                                tokenizer= word_tokenize,   # Use this tokenizer\n",
    "                                stop_words = 'english')     # remove english stopwords\n",
    "\n",
    "bows_counter.fit(corpus)\n",
    "features = bows_counter.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dad57f",
   "metadata": {},
   "source": [
    "The code above implements a count vectorizer that tokenizes words at 1-gram, removes stop words, and creates a one-hot encoding feature set. We can look at the results of our feature conversion by transforming the features into a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57c8d710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>computers</th>\n",
       "      <th>did</th>\n",
       "      <th>food</th>\n",
       "      <th>fun</th>\n",
       "      <th>great</th>\n",
       "      <th>like</th>\n",
       "      <th>love</th>\n",
       "      <th>movie</th>\n",
       "      <th>prefer</th>\n",
       "      <th>programming</th>\n",
       "      <th>python</th>\n",
       "      <th>r</th>\n",
       "      <th>restaurant</th>\n",
       "      <th>use</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   computers  did  food  fun  great  like  love  movie  prefer  programming  \\\n",
       "0          0    0     1    0      1     0     0      0       0            0   \n",
       "1          0    0     0    0      0     0     1      0       0            1   \n",
       "2          0    0     0    0      0     0     0      0       1            0   \n",
       "3          1    0     0    1      0     0     0      0       0            0   \n",
       "4          0    1     0    0      0     1     0      1       0            0   \n",
       "\n",
       "   python  r  restaurant  use  \n",
       "0       0  0           1    0  \n",
       "1       1  0           0    0  \n",
       "2       1  1           0    0  \n",
       "3       0  0           0    1  \n",
       "4       0  0           0    0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df = pd.DataFrame(features, columns=bows_counter.get_feature_names_out())\n",
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20feee64",
   "metadata": {},
   "source": [
    "Notice that the dataframe has 1-gram tokens and an encoding that shows whether a document contains the token. This set of features can help us model the sentiment of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360ad8e5",
   "metadata": {},
   "source": [
    "Another thing to notice is that the matrix can be quite sparse depending on the number of vocabularies and their relative frequency. Therefore, it may be useful to limit n-grams and use features using frequency thresholds.\n",
    "\n",
    "    Term Frequency Inverse Document Frequency a.k.a TF-IDF \"TF-IDF is a commonly used weighting technique that assigns weights reflecting the importance of a word to a document. The basis of this technique is the idea that if a word appears frequently across all documents, it is less likely to hold significant information about any specific document. On the other hand, words that appear frequently in one or a few documents and rarely across all documents are considered to have specific importance and should be assigned higher weights.\n",
    "\n",
    "The mathematical expression of tf-idf (in one of the many forms) is:\n",
    "\n",
    "tfidf = FREQUENCY(t,d) *log( total documents/ total documents containing the term)\n",
    "\n",
    "\n",
    "It is simply the multiplication of the number of times a word appears in a document by the logarithm of the total number of documents divided by the number of documents that contain the word\n",
    "\n",
    "Intuitively, high-frequency words that appear in nearly all documents are weighted by the logarithm of 1 (log1), resulting in a weight of zero. Conversely, words with high frequency within a specific document and low frequency across the corpus will have a higher weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42b8cf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marshal/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer( analyzer='word',          # Word level vectorizer\n",
    "                                    lowercase=True,           # Lowercase the text\n",
    "                                    tokenizer= word_tokenize) # Use this tokenizer)\n",
    "\n",
    "tfidf_vectorizer.fit(corpus)\n",
    "tfidf_features = tfidf_vectorizer.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6cc8e912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>computers</th>\n",
       "      <th>did</th>\n",
       "      <th>food</th>\n",
       "      <th>fun</th>\n",
       "      <th>great</th>\n",
       "      <th>had</th>\n",
       "      <th>i</th>\n",
       "      <th>like</th>\n",
       "      <th>love</th>\n",
       "      <th>movie</th>\n",
       "      <th>not</th>\n",
       "      <th>prefer</th>\n",
       "      <th>programming</th>\n",
       "      <th>python</th>\n",
       "      <th>r</th>\n",
       "      <th>restaurant</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>use</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.463693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.463693</td>\n",
       "      <td>0.463693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.463693</td>\n",
       "      <td>0.374105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380406</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.568014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.568014</td>\n",
       "      <td>0.458270</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.345822</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.416607</td>\n",
       "      <td>0.516374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.416607</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.463693</td>\n",
       "      <td>0.463693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.463693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.374105</td>\n",
       "      <td>0.463693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.442832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296570</td>\n",
       "      <td>0.442832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.442832</td>\n",
       "      <td>0.442832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.357274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        are  computers       did      food       fun     great       had  \\\n",
       "0  0.000000   0.000000  0.000000  0.463693  0.000000  0.463693  0.463693   \n",
       "1  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.463693   0.463693  0.000000  0.000000  0.463693  0.000000  0.000000   \n",
       "4  0.000000   0.000000  0.442832  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "          i      like      love     movie       not    prefer  programming  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000     0.000000   \n",
       "1  0.380406  0.000000  0.568014  0.000000  0.000000  0.000000     0.568014   \n",
       "2  0.345822  0.000000  0.000000  0.000000  0.000000  0.516374     0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000     0.000000   \n",
       "4  0.296570  0.442832  0.000000  0.442832  0.442832  0.000000     0.000000   \n",
       "\n",
       "     python         r  restaurant       the        to       use  \n",
       "0  0.000000  0.000000    0.463693  0.374105  0.000000  0.000000  \n",
       "1  0.458270  0.000000    0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.416607  0.516374    0.000000  0.000000  0.416607  0.000000  \n",
       "3  0.000000  0.000000    0.000000  0.000000  0.374105  0.463693  \n",
       "4  0.000000  0.000000    0.000000  0.357274  0.000000  0.000000  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df = pd.DataFrame(tfidf_features, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9518ef",
   "metadata": {},
   "source": [
    "Notice that we now have weights computed. Because we are using a small corpus, the disparity of the weights is not highly visible but it does show. We will implement this for our review data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b869c0b8",
   "metadata": {},
   "source": [
    "## 4. CountVectorizer and TfidfVectorizer\n",
    "\n",
    "We can use a vectorizer for text outside of the training data. It will create a vector corresponding to the column names and adds a tfidf value if the word is present in the column and zero otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9208b844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bows_counter.transform(['python programming is great']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c94b262a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.61418897, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.61418897, 0.49552379,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.transform(['python programming is great']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee90834",
   "metadata": {},
   "source": [
    "## 4.1 Features to Matrix\n",
    "\n",
    "The final step is to apply the tfidf vectorize to the dataset to obtain the feature and convert features into a matrix that can be ingested into a model for training. The example below demonstrates this implementation using the Amazon product reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "595471b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marshal/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:591: FutureWarning: Passing an int for a boolean parameter is deprecated in version 1.2 and won't be supported anymore in version 1.4.\n",
      "  warnings.warn(\n",
      "/home/marshal/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abd</th>\n",
       "      <th>abd walk</th>\n",
       "      <th>abit</th>\n",
       "      <th>abit rough</th>\n",
       "      <th>abl</th>\n",
       "      <th>abl tri</th>\n",
       "      <th>abl wear</th>\n",
       "      <th>absolut</th>\n",
       "      <th>absolut comfort</th>\n",
       "      <th>absolut favorit</th>\n",
       "      <th>...</th>\n",
       "      <th>zing</th>\n",
       "      <th>zing need</th>\n",
       "      <th>zumba</th>\n",
       "      <th>zumba class</th>\n",
       "      <th>zumba heel</th>\n",
       "      <th>zumba hip</th>\n",
       "      <th>zumba realli</th>\n",
       "      <th>zumba trial</th>\n",
       "      <th>zumba yeah</th>\n",
       "      <th>zumba zumba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4033 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abd  abd walk  abit  abit rough  abl  abl tri  abl wear  absolut  \\\n",
       "0  0.0       0.0   0.0         0.0  0.0      0.0       0.0      0.0   \n",
       "1  0.0       0.0   0.0         0.0  0.0      0.0       0.0      0.0   \n",
       "2  0.0       0.0   0.0         0.0  0.0      0.0       0.0      0.0   \n",
       "3  0.0       0.0   0.0         0.0  0.0      0.0       0.0      0.0   \n",
       "4  0.0       0.0   0.0         0.0  0.0      0.0       0.0      0.0   \n",
       "\n",
       "   absolut comfort  absolut favorit  ...  zing  zing need  zumba  zumba class  \\\n",
       "0              0.0              0.0  ...   0.0        0.0    0.0          0.0   \n",
       "1              0.0              0.0  ...   0.0        0.0    0.0          0.0   \n",
       "2              0.0              0.0  ...   0.0        0.0    0.0          0.0   \n",
       "3              0.0              0.0  ...   0.0        0.0    0.0          0.0   \n",
       "4              0.0              0.0  ...   0.0        0.0    0.0          0.0   \n",
       "\n",
       "   zumba heel  zumba hip  zumba realli  zumba trial  zumba yeah  zumba zumba  \n",
       "0         0.0        0.0           0.0          0.0         0.0          0.0  \n",
       "1         0.0        0.0           0.0          0.0         0.0          0.0  \n",
       "2         0.0        0.0           0.0          0.0         0.0          0.0  \n",
       "3         0.0        0.0           0.0          0.0         0.0          0.0  \n",
       "4         0.0        0.0           0.0          0.0         0.0          0.0  \n",
       "\n",
       "[5 rows x 4033 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "review_tfidf_vectorizer = TfidfVectorizer( #max_feautures = 1000,        # Return the top 1000 features\n",
    "                                        analyzer='word',              # Word level vectorizer\n",
    "                                        lowercase=True,               # Lower case the text\n",
    "                                        min_df = 5,                   # Use tokens that appear at least 5 times\n",
    "                                        ngram_range=(1, 2),           # Create 1 n-grams\n",
    "                                        tokenizer= word_tokenize,     # Use this tokenizer\n",
    "                                        stop_words = 'english',       # remove english stopwords \n",
    "                                        sublinear_tf=1, smooth_idf=1, use_idf=1) # Additional Features\n",
    "\n",
    "review_tfidf_vectorizer.fit(fashion_data.clean_review)\n",
    "features_df = pd.DataFrame( review_tfidf_vectorizer.transform(fashion_data.clean_review).toarray(), \n",
    "                            columns=review_tfidf_vectorizer.get_feature_names_out())\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38da3034",
   "metadata": {},
   "source": [
    "## 5. Pickling Vectorizer and Features Dataframe\n",
    "\n",
    "Working with NLP often means working with sparse datasets and vectorizer classes which take a long time to run. Pickling these objects and datasets can help implement checkpoints to save time and compute. To complete this note, we implement pickling for the features dataframe and tfidt_vectorizer we developed."
   ]
  },
  {
   "cell_type": "raw",
   "id": "774225da",
   "metadata": {},
   "source": [
    "# import pickle\n",
    "\n",
    "# vectorizer pickling\n",
    "vectorizer_object = open( 'review_tfidf_vectorizer.pk' , 'wb')\n",
    "pickle.dump( review_tfidf_vectorizer , vectorizer_object    )\n",
    "vectorizer_object.close()\n",
    "\n",
    "# features_data pickling\n",
    "feature_object = open( 'features.pk' , 'wb')\n",
    "pickle.dump( features_df , feature_object )\n",
    "feature_object.close()\n",
    "\n",
    "# sentiments_pickling\n",
    "sentiment_object = open( 'sentiment.pk' , 'wb')\n",
    "pickle.dump( review_data['sentiment'] , sentiment_object )\n",
    "sentiment_object.close()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec67570",
   "metadata": {},
   "source": [
    "## Part 3: Sentiment Classification with NaiveBayes and SVM - Linear Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88f5608",
   "metadata": {},
   "source": [
    "In Part 2, we developed feature extraction techniques like Bag of Words, N-grams, and tdidf to create a feature list from the Amazon product review dataset. On this note, we move on to building a sentiment classifier using Naive Bayes and Support Vector Machines. Specifically, we cover:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de3639c",
   "metadata": {},
   "source": [
    "### Introducing Review Dataset\n",
    "\n",
    "    1.Preprocessing Restaurant Reviews\n",
    "    2.Train and Test Split\n",
    "\n",
    "### Classification Pipeline and Modeling\n",
    "\n",
    "    1.Naive Bayes Classifier\n",
    "    2.Classifying New Text\n",
    "    3.Support Vector Machine - Linear Classifier\n",
    "    \n",
    "### Dataset: Appliances Amazon review\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26a2fae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>style</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>08 22, 2013</td>\n",
       "      <td>A34A1UP40713F8</td>\n",
       "      <td>B00009W3I4</td>\n",
       "      <td>{'Style:': ' Dryer Vent'}</td>\n",
       "      <td>James. Backus</td>\n",
       "      <td>I like this as a vent as well as something tha...</td>\n",
       "      <td>Great product</td>\n",
       "      <td>1377129600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>02 8, 2016</td>\n",
       "      <td>A1AHW6I678O6F2</td>\n",
       "      <td>B00009W3PA</td>\n",
       "      <td>{'Size:': ' 6-Foot'}</td>\n",
       "      <td>kevin.</td>\n",
       "      <td>good item</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1454889600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>08 5, 2015</td>\n",
       "      <td>A8R48NKTGCJDQ</td>\n",
       "      <td>B00009W3PA</td>\n",
       "      <td>{'Size:': ' 6-Foot'}</td>\n",
       "      <td>CDBrannom</td>\n",
       "      <td>Fit my new LG dryer perfectly.</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1438732800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>04 24, 2015</td>\n",
       "      <td>AR3OHHHW01A8E</td>\n",
       "      <td>B00009W3PA</td>\n",
       "      <td>{'Size:': ' 6-Foot'}</td>\n",
       "      <td>Calvin E Reames</td>\n",
       "      <td>Good value for electric dryers</td>\n",
       "      <td>Perfect size</td>\n",
       "      <td>1429833600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>03 21, 2015</td>\n",
       "      <td>A2CIEGHZ7L1WWR</td>\n",
       "      <td>B00009W3PA</td>\n",
       "      <td>{'Size:': ' 6-Foot'}</td>\n",
       "      <td>albert j. kong</td>\n",
       "      <td>Price and delivery was excellent.</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1426896000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified   reviewTime      reviewerID        asin  \\\n",
       "0        5      True  08 22, 2013  A34A1UP40713F8  B00009W3I4   \n",
       "1        5      True   02 8, 2016  A1AHW6I678O6F2  B00009W3PA   \n",
       "2        5      True   08 5, 2015   A8R48NKTGCJDQ  B00009W3PA   \n",
       "3        5      True  04 24, 2015   AR3OHHHW01A8E  B00009W3PA   \n",
       "4        5      True  03 21, 2015  A2CIEGHZ7L1WWR  B00009W3PA   \n",
       "\n",
       "                       style     reviewerName  \\\n",
       "0  {'Style:': ' Dryer Vent'}    James. Backus   \n",
       "1       {'Size:': ' 6-Foot'}           kevin.   \n",
       "2       {'Size:': ' 6-Foot'}        CDBrannom   \n",
       "3       {'Size:': ' 6-Foot'}  Calvin E Reames   \n",
       "4       {'Size:': ' 6-Foot'}   albert j. kong   \n",
       "\n",
       "                                          reviewText        summary  \\\n",
       "0  I like this as a vent as well as something tha...  Great product   \n",
       "1                                          good item     Five Stars   \n",
       "2                     Fit my new LG dryer perfectly.     Five Stars   \n",
       "3                     Good value for electric dryers   Perfect size   \n",
       "4                  Price and delivery was excellent.     Five Stars   \n",
       "\n",
       "   unixReviewTime vote image  \n",
       "0      1377129600  NaN   NaN  \n",
       "1      1454889600  NaN   NaN  \n",
       "2      1438732800  NaN   NaN  \n",
       "3      1429833600  NaN   NaN  \n",
       "4      1426896000  NaN   NaN  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d6809df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Appliances_data = review_data[['reviewText']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dcda7334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I like this as a vent as well as something tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>good item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fit my new LG dryer perfectly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good value for electric dryers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Price and delivery was excellent.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText\n",
       "0  I like this as a vent as well as something tha...\n",
       "1                                          good item\n",
       "2                     Fit my new LG dryer perfectly.\n",
       "3                     Good value for electric dryers\n",
       "4                  Price and delivery was excellent."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Appliances_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6d0edf98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2277"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Appliances_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f043f6",
   "metadata": {},
   "source": [
    "## Dataset: Restaurant Reviews\n",
    "\n",
    "If you have been following the notebooks, you will notice that using the Amazon Product Reviews dataset is computationally expensive and sometimes prohibitive. Cases of running out of memory are common. To mitigate this, I introduce a lighter dataset called restaurant_reviews. It is available here: Restaurant Review Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7493f0b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_data = pd.read_csv('restaurant_reviews.tsv', sep='\\t')\n",
    "rest_data.head()                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "827e1374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rest_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "275c5dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,\n",
       " Liked\n",
       " 1    500\n",
       " 0    500\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rest_data), rest_data.Liked.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c32477",
   "metadata": {},
   "source": [
    "\n",
    "## Preprocessing: Cleaning and Stemming Reviews\n",
    "\n",
    "The code below implements cleaning of the text and stemming to return root words in each review. The outcome is shorter sentences with a high indication of sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "184df9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>wow love place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>crust good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>not tasti textur nasti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>stop late may bank holiday rick steve recommen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>the select menu great price</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  \\\n",
       "0                           Wow... Loved this place.   \n",
       "1                                 Crust is not good.   \n",
       "2          Not tasty and the texture was just nasty.   \n",
       "3  Stopped by during the late May bank holiday of...   \n",
       "4  The selection on the menu was great and so wer...   \n",
       "\n",
       "                                        clean_review  \n",
       "0                                     wow love place  \n",
       "1                                         crust good  \n",
       "2                             not tasti textur nasti  \n",
       "3  stop late may bank holiday rick steve recommen...  \n",
       "4                        the select menu great price  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "def cleaningText(text):\n",
    "    \"\"\"\n",
    "    Text Cleaning:\n",
    "        - Remove Punctuation\n",
    "        - Remove Numbers\n",
    "        - Tokenize Text\n",
    "        - Stem Text\n",
    "        - Remove Stopwords\n",
    "    \"\"\" \n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text) # Remove Punctuation\n",
    "    text = re.sub(\"[0-9]+\", \"\", text) # Remove Numbers\n",
    "    text = [ porter_stemmer.stem(word.lower()) for word in word_tokenize(text) if word not in stopwords.words('english') ]\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "rest_data['clean_review'] = rest_data.Review.apply(lambda x: cleaningText( str(x) ))\n",
    "rest_data[[ 'Review', 'clean_review' ]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde8b993",
   "metadata": {},
   "source": [
    "## Preproceing: Features to Matrix\n",
    "\n",
    "The code below convert the clean_review column into 1-2 ngram features using TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "74a0a894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marshal/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:591: FutureWarning: Passing an int for a boolean parameter is deprecated in version 1.2 and won't be supported anymore in version 1.4.\n",
      "  warnings.warn(\n",
      "/home/marshal/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absolut</th>\n",
       "      <th>alway</th>\n",
       "      <th>amaz</th>\n",
       "      <th>ambianc</th>\n",
       "      <th>anoth</th>\n",
       "      <th>anoth minut</th>\n",
       "      <th>anytim</th>\n",
       "      <th>anytim soon</th>\n",
       "      <th>area</th>\n",
       "      <th>arriv</th>\n",
       "      <th>...</th>\n",
       "      <th>wast</th>\n",
       "      <th>watch</th>\n",
       "      <th>way</th>\n",
       "      <th>went</th>\n",
       "      <th>wine</th>\n",
       "      <th>wonder</th>\n",
       "      <th>worst</th>\n",
       "      <th>worth</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 253 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   absolut  alway  amaz  ambianc  anoth  anoth minut  anytim  anytim soon  \\\n",
       "0      0.0    0.0   0.0      0.0    0.0          0.0     0.0          0.0   \n",
       "1      0.0    0.0   0.0      0.0    0.0          0.0     0.0          0.0   \n",
       "2      0.0    0.0   0.0      0.0    0.0          0.0     0.0          0.0   \n",
       "3      0.0    0.0   0.0      0.0    0.0          0.0     0.0          0.0   \n",
       "4      0.0    0.0   0.0      0.0    0.0          0.0     0.0          0.0   \n",
       "\n",
       "   area  arriv  ...  wast  watch  way  went  wine  wonder  worst  worth  \\\n",
       "0   0.0    0.0  ...   0.0    0.0  0.0   0.0   0.0     0.0    0.0    0.0   \n",
       "1   0.0    0.0  ...   0.0    0.0  0.0   0.0   0.0     0.0    0.0    0.0   \n",
       "2   0.0    0.0  ...   0.0    0.0  0.0   0.0   0.0     0.0    0.0    0.0   \n",
       "3   0.0    0.0  ...   0.0    0.0  0.0   0.0   0.0     0.0    0.0    0.0   \n",
       "4   0.0    0.0  ...   0.0    0.0  0.0   0.0   0.0     0.0    0.0    0.0   \n",
       "\n",
       "   wrong  year  \n",
       "0    0.0   0.0  \n",
       "1    0.0   0.0  \n",
       "2    0.0   0.0  \n",
       "3    0.0   0.0  \n",
       "4    0.0   0.0  \n",
       "\n",
       "[5 rows x 253 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer( #max_feautures = 1000,        # Return the top 1000 features\n",
    "                                    analyzer='word',              # Word level vectorizer\n",
    "                                    lowercase=True,               # Lower case the text\n",
    "                                    min_df = 5,                   # Use tokens that appear at least 5 times\n",
    "                                    ngram_range=(1, 2),           # Create 1 n-grams\n",
    "                                    tokenizer= word_tokenize,     # Use this tokenizer\n",
    "                                    stop_words = 'english',       # remove english stopwords \n",
    "                                    sublinear_tf=1, smooth_idf=1, use_idf=1) # Additional Features\n",
    "\n",
    "tfidf_vectorizer.fit(rest_data.clean_review)\n",
    "features = pd.DataFrame( tfidf_vectorizer.transform(rest_data.clean_review).toarray(), \n",
    "                         columns=tfidf_vectorizer.get_feature_names_out())\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27500bb7",
   "metadata": {},
   "source": [
    "## Train and Test Split\n",
    "\n",
    "The code below implement a 70-30 percent Train to Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9cb3c4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Liked\n",
       " 1    350\n",
       " 0    350\n",
       " Name: count, dtype: int64,\n",
       " Liked\n",
       " 1    150\n",
       " 0    150\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split( features, rest_data.Liked, test_size=.30, stratify=rest_data.Liked, random_state=42)\n",
    "y_train.value_counts(), y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1bd95f",
   "metadata": {},
   "source": [
    "\n",
    "## Naive Bayes Model Implementation in Python\n",
    "\n",
    "The code below initializes a NaiveBayes Model with a laplace estimator parameter at .3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "11f11070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "    \n",
    "naive_bayes_model = MultinomialNB(alpha=.3, fit_prior=True)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3eb7cc",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "17a2d7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB(alpha=0.3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB(alpha=0.3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB(alpha=0.3)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9d3091",
   "metadata": {},
   "source": [
    "\n",
    "## Train and Test Assessment\n",
    "\n",
    "After training the model, we can compute the train and the test error. This helps us determine how good or not-so-good our model did on new reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5325c66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.85\n",
      "Test Accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Training Accuracy:\", round( accuracy_score(naive_bayes_model.predict(x_train), y_train ), 2) )\n",
    "print(\"Test Accuracy:\", round( accuracy_score(naive_bayes_model.predict(x_test), y_test ), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeef9af",
   "metadata": {},
   "source": [
    "The training and test accuracies are not bad for a basic model. We should expect it to perform well most of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1d218e",
   "metadata": {},
   "source": [
    "\n",
    "## Predicting New Reviews\n",
    "\n",
    "To run the model on reviews outside of the test and train set, we must implement the same preprocessing steps and vectorization. Below is an example of the implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "75810b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marshal/anaconda3/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but MultinomialNB was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/marshal/anaconda3/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but MultinomialNB was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1]), array([0]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "pos_text = tfidf_vectorizer.transform( [cleaningText(\"The restaurant had great food\")] ).toarray()\n",
    "neg_text = tfidf_vectorizer.transform( [cleaningText(\"The restaurant had terrible food\")] ).toarray()\n",
    "\n",
    "naive_bayes_model.predict(pos_text), naive_bayes_model.predict(neg_text)\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3258aa36",
   "metadata": {},
   "source": [
    "As we can see with the simple examples above, the model performs decently, prediction 1 for positive and 0 for negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467cdd46",
   "metadata": {},
   "source": [
    "\n",
    "## Sentiment Classification with Support Vector Machine\n",
    "\n",
    "Support Vector Machine is a family of classification algorithms that perform classification by determining the hyperplane that separates the classes in question. SVMs are linear classifiers that can be modified to take a variety of linear functions as a way to separate two or more classes by determining the hyperplane that maximized the distance between observations across classes.\n",
    "\n",
    "SVM turns out to be very effective in working with sparse data as they are linear. Given that we have very sparse metrics of features, let's use SVM to determine the sentiment.\n",
    "\n",
    "Notice that SVM has the following tuning parameters:\n",
    "\n",
    "    1. Kernel: Specifies a kernel formula to use when determining the decision boundary\n",
    "    2. Gamma: Weighting based on observation distance from the decision boundary\n",
    "    3. C Parameter: Balance between model complexity (correct classification) and smooth boundary Below is the   implementation in Python:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dad065ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_linear =  SVC( C=1,                # Setting C at default parameter\n",
    "                   kernel='linear',    # Using linear kernel transformation \n",
    "                   gamma=100,          # Setting Gamma at 100\n",
    "                   probability=True,\n",
    "                   random_state= 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531b4057",
   "metadata": {},
   "source": [
    "\n",
    "## Fitting the model\n",
    "\n",
    "Much like we did with Naive Bayes, we fit the model for SVM using the fit() method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a5060aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=1, gamma=100, kernel=&#x27;linear&#x27;, probability=True, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1, gamma=100, kernel=&#x27;linear&#x27;, probability=True, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=1, gamma=100, kernel='linear', probability=True, random_state=42)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_linear.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b1596be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.87\n",
      "Test Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy:\", round( accuracy_score(svm_linear.predict(x_train), y_train), 2) )\n",
    "print(\"Test Accuracy:\", round( accuracy_score(svm_linear.predict(x_test), y_test ), 2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a60fd4",
   "metadata": {},
   "source": [
    "\n",
    "## Predicting with SVM Model\n",
    "\n",
    "We can now test our model with data outside of the training and testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "56911da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marshal/anaconda3/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/marshal/anaconda3/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/marshal/anaconda3/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/marshal/anaconda3/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0]),\n",
       " array([[0.69524773, 0.30475227]]),\n",
       " array([0]),\n",
       " array([[0.79130595, 0.20869405]]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_text = tfidf_vectorizer.transform( [cleaningText(\"The food was okay\")] ).toarray()\n",
    "neg_text = tfidf_vectorizer.transform( [cleaningText(\"I did not like the food\")] ).toarray()\n",
    "\n",
    "svm_linear.predict(pos_text), svm_linear.predict_proba(pos_text), svm_linear.predict(neg_text), svm_linear.predict_proba(neg_text)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da37463e",
   "metadata": {},
   "source": [
    "We see that our model is doing a reasonably good job of predicting sentiment and their associated probabilities. For example, the negative sentiment has higher confidence that the neutral sentiment. Notice that neutral in this case fall on the negative classification because of our binary target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "44fe6650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "      <td>wow love place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "      <td>crust good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "      <td>not tasti textur nasti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "      <td>stop late may bank holiday rick steve recommen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>the select menu great price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>I think food should have flavor and texture an...</td>\n",
       "      <td>0</td>\n",
       "      <td>i think food flavor textur lack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Appetite instantly gone.</td>\n",
       "      <td>0</td>\n",
       "      <td>appetit instantli gone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Overall I was not impressed and would not go b...</td>\n",
       "      <td>0</td>\n",
       "      <td>overal i impress would go back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The whole experience was underwhelming, and I ...</td>\n",
       "      <td>0</td>\n",
       "      <td>the whole experi underwhelm i think go ninja s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Then, as if I hadn't wasted enough of my life ...</td>\n",
       "      <td>0</td>\n",
       "      <td>then i wast enough life pour salt wound draw t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Review  Liked  \\\n",
       "0                             Wow... Loved this place.      1   \n",
       "1                                   Crust is not good.      0   \n",
       "2            Not tasty and the texture was just nasty.      0   \n",
       "3    Stopped by during the late May bank holiday of...      1   \n",
       "4    The selection on the menu was great and so wer...      1   \n",
       "..                                                 ...    ...   \n",
       "995  I think food should have flavor and texture an...      0   \n",
       "996                           Appetite instantly gone.      0   \n",
       "997  Overall I was not impressed and would not go b...      0   \n",
       "998  The whole experience was underwhelming, and I ...      0   \n",
       "999  Then, as if I hadn't wasted enough of my life ...      0   \n",
       "\n",
       "                                          clean_review  \n",
       "0                                       wow love place  \n",
       "1                                           crust good  \n",
       "2                               not tasti textur nasti  \n",
       "3    stop late may bank holiday rick steve recommen...  \n",
       "4                          the select menu great price  \n",
       "..                                                 ...  \n",
       "995                    i think food flavor textur lack  \n",
       "996                             appetit instantli gone  \n",
       "997                     overal i impress would go back  \n",
       "998  the whole experi underwhelm i think go ninja s...  \n",
       "999  then i wast enough life pour salt wound draw t...  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa2f7b1",
   "metadata": {},
   "source": [
    "Restaurant Dataset Review "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aa6df3",
   "metadata": {},
   "source": [
    "## Part 4: Modern Feature Engineering - Distributed Representation for Text Modelling\n",
    "\n",
    "So far we have looked at traditional techniques that include using word frequency and weighting to determine the features from text and using those features to build classification models. On this note, we explore distributed representation as a way of generating features from words. In this notebook, we will look at the implementation of distributed representation techniques. More specifically\n",
    "Word2Vec Implementation with Gensim:\n",
    "\n",
    "    1. Continuous Bag of Words Model\n",
    "    2. Skip-Gram Model\n",
    "    3. Gensim Vocabularity Object\n",
    "\n",
    "Word Vectors to Feature Matrix\n",
    "\n",
    "    1. Averaging Word Vectors\n",
    "    2. Building a vectorizer for new text\n",
    "\n",
    "Dataset: Restaurant Reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d558f8",
   "metadata": {},
   "source": [
    "We will need to clean them up so that we have a sequence of words that we can build our word2vector representation. Before we go into the word2vec, let's use Keras to tokenize the sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "df152e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wow', 'loved', 'this', 'place'],\n",
       " ['crust', 'is', 'not', 'good'],\n",
       " ['not', 'tasty', 'and', 'the', 'texture', 'was', 'just', 'nasty']]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "    \n",
    "corpus_tokens = [text_to_word_sequence(review) for review in rest_data.Review.values ]\n",
    "corpus_tokens[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3e4aea",
   "metadata": {},
   "source": [
    "Notice that in the above example, we have created a list of documents in which every document is a set of tokens in the documents. This is the input we will need to use for our word2vec implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111caff4",
   "metadata": {},
   "source": [
    "\n",
    "## What is Word2Vec?\n",
    "\n",
    "So far we have generally described distributed representation. The simplified understanding of distributed representation is the idea that words that have the same meaning tend to appear within the same context. More broadly, we distributed representation provides a way assign vectors to words based on the context such that words within a similar context are contained within the same vector space.\n",
    "\n",
    "For more information, see stanford's lecture: https://www.youtube.com/watch?v=ERibwqs9p38\n",
    "\n",
    "There are two main ways of computing word vectors:\n",
    "\n",
    "    1. CBOW - Continuous Bag of Words\n",
    "    2. Skip-Gram Model\n",
    "\n",
    "## 1. CBOW - Continuous Bag of Words\n",
    "\n",
    "Suppose we have some text: \"generalized linear models have link functions that enable flexibility beyond that of ordinary least squares\". Also, suppose that we want to predict the word vector for the word \"models\", the continuous bag of words uses the context words: \"generalized linear ___ have link\" to predict the target word. Given the corpus may contain other texts with similar context, then we can train a cbow model to provide the most probable word for the text above.\n",
    "\n",
    "If you are interested in understanding the mechanics of training word2vec, it may be useful to visit the Stanford link above for more technical details of the training algorithms available for this estimation.\n",
    "\n",
    "In Python, I use the gensim package to compute the words vectors for the review corpus using cbow model\n",
    "Before we implement cbow word2vec, we need to determine the following:\n",
    "\n",
    "    1. size of vector: The size of the vectors for every word in the corpus\n",
    "    2. window size: The size of context words to use in computing the vector\n",
    "\n",
    "\n",
    "In this example, let's use a window of 5 and a vector of size 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "926b80a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "vector_size = 10\n",
    "window_size = 5\n",
    "\n",
    "cbow_model = Word2Vec(  sentences= corpus_tokens,\n",
    "                        vector_size = vector_size,  # Setting the vector size\n",
    "                        window =window_size,  # Setting the Window size\n",
    "                        sg=0,                                 # Initialize CBOW\n",
    "                        min_count = 2,               # Minimum word count\n",
    "                        sample=.0000001 )    # Lower Weighting/Downsampling of frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3e9ed3",
   "metadata": {},
   "source": [
    "The model has been trained. We can now extract a vector of size 10 using the 5 nearest words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "026f9760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07817571, -0.09510187, -0.00205531,  0.03469197, -0.00938972,\n",
       "        0.08381772,  0.09010784,  0.06536506, -0.00711621,  0.07710405],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv['good']\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbfb556",
   "metadata": {},
   "source": [
    "## 1.2. CBOW Similarity\n",
    "\n",
    "One of the advantages of using a CBOW model is we can then compute word similarity within the corpus. The cosine similarity method is used for this calculation.\n",
    "\n",
    "Using the gensim package, we can call the most_similar method for any word as shown in the example below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "62be232b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mayo', 0.8547266721725464),\n",
       " ('pricing', 0.8052123785018921),\n",
       " ('twice', 0.8021304607391357),\n",
       " ('40', 0.7712088227272034),\n",
       " ('so', 0.7700384855270386),\n",
       " ('party', 0.752712070941925),\n",
       " ('liked', 0.735294759273529),\n",
       " ('say', 0.7321372628211975),\n",
       " ('terrible', 0.7217848896980286),\n",
       " ('part', 0.7145952582359314)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv.most_similar('amazing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2857129e",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Skip-Gram Model\n",
    "\n",
    "The Skip-Gram model is trained to perform the reverse function of the CBOW model. That is, while the cbow model predicts the target word given context words, the Skip-Gram model predicts context words based on the presence of the target word.\n",
    "\n",
    "In terms of the general implementation of skip-gram in gensim, we will only activate the skip-gram argument. Let's see the implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c6bd586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "skipgram_model = Word2Vec(  sentences= corpus_tokens,\n",
    "                            vector_size = 20,    # Setting the vector size\n",
    "                            window = 5,          # Setting the Window size\n",
    "                            min_count = 2,       # Minimum word count\n",
    "                            sg = 1,              # Initialize Skip Gram\n",
    "                            sample=.0000001 )    # Lower Weighting/Downsampling of frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b07916",
   "metadata": {},
   "source": [
    "Just like with CBOW, we can generate a word vector for each word in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2c86e5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04121573,  0.04649187, -0.00098335, -0.00983143,  0.02302161,\n",
       "       -0.02047365,  0.01371725,  0.03470667,  0.03032966, -0.03756103,\n",
       "        0.04690514,  0.0233656 ,  0.01983496, -0.03122055,  0.0423056 ,\n",
       "       -0.01075502,  0.04413366, -0.02680901, -0.04064848,  0.03411919],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram_model.wv['good']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0594e3d4",
   "metadata": {},
   "source": [
    "## 3. Gensim Vocabulary\n",
    "\n",
    "Suppose we have a new text input that may have new words that our models have not seen yet, it is therefore impossible to return the vectors for those words. It is always important to know how to access the vocabulary in the model so that you can provide an alternative to new words are you are trying to leverage the models for feature extraction.\n",
    "\n",
    "The gensim vocabulary is a dictionary of all words with the vector object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "66c7516f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with', 'had', 'great', 'that', 'be', 'so', 'were', 'are', 'but', 'have']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = skipgram_model.wv.index_to_key\n",
    "vocab[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7e86b8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "897"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243d04c5",
   "metadata": {},
   "source": [
    "## 4. Convert Word Vectors to Features\n",
    "\n",
    "We have seen earlier how to implement bag of words and tfidf feature extraction. With those techniques, every word has a single value as a feature. With word vectors, every word has a vector of features so we will need to find a way to summarize the features such that every document has one vector representing all the tokens in the document. One simple approach is to sum the vectors and average them by the count of the words/tokens.\n",
    "\n",
    "Let's see this in implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7a602527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def avg_word_vectors(words, model, vocabulary, feature_size):\n",
    "    \n",
    "    feature_vector = np.zeros((feature_size,), dtype='float64')\n",
    "    word_count = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            word_count += 1\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "            \n",
    "        if word_count:\n",
    "            feature_vector = np.divide(feature_vector, word_count)\n",
    "            \n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3bca24",
   "metadata": {},
   "source": [
    "Testing the function on a sample of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fd9f1514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00381702, -0.00316857,  0.00247694, -0.00598015,  0.02107186,\n",
       "        0.0180915 , -0.00301856,  0.01412969, -0.01600609,  0.01004132,\n",
       "        0.0075589 , -0.00288089,  0.01646154, -0.00441363,  0.01078511,\n",
       "       -0.01157882,  0.02155005,  0.01900969, -0.01142925,  0.00739343])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [\"This\", 'is', 'delicious', 'food']\n",
    "avg_word_vectors(test, skipgram_model, vocab, 20 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ba7201",
   "metadata": {},
   "source": [
    "\n",
    "## Averaging across the full dataset\n",
    "\n",
    "Now let's convert the averaged vectors to a full dataframe with words as the predictors/features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1c66442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word_vectorizer(corpus, model, feature_size):\n",
    "    vocabulary = set(model.wv.index_to_key)\n",
    "    features = [ avg_word_vectors(text, model, vocabulary, feature_size) for text in corpus]\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "skipgram_feaures = avg_word_vectorizer(corpus_tokens, skipgram_model, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e81ed5",
   "metadata": {},
   "source": [
    "## Converting Vectors into a Feature Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3af39c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002200</td>\n",
       "      <td>-0.015287</td>\n",
       "      <td>-0.015425</td>\n",
       "      <td>-0.001512</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>-0.004541</td>\n",
       "      <td>-0.002240</td>\n",
       "      <td>-0.010102</td>\n",
       "      <td>0.013118</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>-0.002498</td>\n",
       "      <td>0.004954</td>\n",
       "      <td>0.007783</td>\n",
       "      <td>-0.005989</td>\n",
       "      <td>0.013167</td>\n",
       "      <td>0.012485</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.004616</td>\n",
       "      <td>0.010194</td>\n",
       "      <td>-0.006662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.014202</td>\n",
       "      <td>0.009345</td>\n",
       "      <td>-0.001474</td>\n",
       "      <td>-0.007928</td>\n",
       "      <td>0.008754</td>\n",
       "      <td>-0.004882</td>\n",
       "      <td>0.004632</td>\n",
       "      <td>0.012017</td>\n",
       "      <td>0.003751</td>\n",
       "      <td>-0.007202</td>\n",
       "      <td>0.017074</td>\n",
       "      <td>0.002244</td>\n",
       "      <td>0.002607</td>\n",
       "      <td>-0.011181</td>\n",
       "      <td>0.006813</td>\n",
       "      <td>-0.003037</td>\n",
       "      <td>0.014002</td>\n",
       "      <td>-0.004456</td>\n",
       "      <td>-0.007994</td>\n",
       "      <td>0.012142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000496</td>\n",
       "      <td>-0.003071</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.003619</td>\n",
       "      <td>-0.001678</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>-0.001002</td>\n",
       "      <td>0.002840</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>-0.002766</td>\n",
       "      <td>-0.005903</td>\n",
       "      <td>-0.002449</td>\n",
       "      <td>0.004655</td>\n",
       "      <td>-0.004210</td>\n",
       "      <td>-0.004335</td>\n",
       "      <td>0.003840</td>\n",
       "      <td>-0.005615</td>\n",
       "      <td>-0.003023</td>\n",
       "      <td>-0.004575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.004266</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>-0.002245</td>\n",
       "      <td>-0.002431</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>0.003604</td>\n",
       "      <td>-0.002653</td>\n",
       "      <td>0.003792</td>\n",
       "      <td>0.003001</td>\n",
       "      <td>-0.001501</td>\n",
       "      <td>-0.003569</td>\n",
       "      <td>0.002076</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>-0.002937</td>\n",
       "      <td>-0.003272</td>\n",
       "      <td>-0.000946</td>\n",
       "      <td>0.002147</td>\n",
       "      <td>-0.001985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.001113</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>-0.000484</td>\n",
       "      <td>0.003836</td>\n",
       "      <td>0.003093</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>0.002986</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>-0.001045</td>\n",
       "      <td>0.002020</td>\n",
       "      <td>-0.001210</td>\n",
       "      <td>-0.001275</td>\n",
       "      <td>0.003626</td>\n",
       "      <td>-0.003685</td>\n",
       "      <td>-0.001368</td>\n",
       "      <td>-0.001514</td>\n",
       "      <td>0.001991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.002200 -0.015287 -0.015425 -0.001512  0.001022 -0.004541 -0.002240   \n",
       "1 -0.014202  0.009345 -0.001474 -0.007928  0.008754 -0.004882  0.004632   \n",
       "2  0.000496 -0.003071  0.000526  0.000306  0.003619 -0.001678  0.001542   \n",
       "3 -0.004266  0.001542 -0.002245 -0.002431  0.001924  0.002580  0.003604   \n",
       "4 -0.001113  0.000919 -0.000484  0.003836  0.003093  0.000356  0.001461   \n",
       "\n",
       "         7         8         9         10        11        12        13  \\\n",
       "0 -0.010102  0.013118  0.002800 -0.002498  0.004954  0.007783 -0.005989   \n",
       "1  0.012017  0.003751 -0.007202  0.017074  0.002244  0.002607 -0.011181   \n",
       "2 -0.001002  0.002840  0.003904 -0.002766 -0.005903 -0.002449  0.004655   \n",
       "3 -0.002653  0.003792  0.003001 -0.001501 -0.003569  0.002076  0.002506   \n",
       "4  0.002986  0.000749  0.002564  0.000360 -0.001045  0.002020 -0.001210   \n",
       "\n",
       "         14        15        16        17        18        19  \n",
       "0  0.013167  0.012485  0.009511  0.004616  0.010194 -0.006662  \n",
       "1  0.006813 -0.003037  0.014002 -0.004456 -0.007994  0.012142  \n",
       "2 -0.004210 -0.004335  0.003840 -0.005615 -0.003023 -0.004575  \n",
       "3  0.000086 -0.002937 -0.003272 -0.000946  0.002147 -0.001985  \n",
       "4 -0.001275  0.003626 -0.003685 -0.001368 -0.001514  0.001991  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "skipgram_df = pd.DataFrame(skipgram_feaures)\n",
    "skipgram_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75910915",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "What we have done is reduce all of our text to a dataframe that has 20 features representing the average of all of the word vectors for the word in the text. We can use this matrix for some of the classification tasks that we did before. In the last section, we will cover how to use deep learning for the classification of the sentiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ffa02a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
